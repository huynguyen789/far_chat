{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install poppler-utils tesseract-ocr  unstructured unstructured-client python-dotenv langchain langchain-openai langchain-community faiss-cpu ipython rank-bm25 pydantic httpx unstructured libmagic-dev pydantic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PARSE DATA\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from unstructured_client import UnstructuredClient\n",
    "from unstructured_client.models import shared\n",
    "from unstructured_client.models.errors import SDKError\n",
    "from unstructured.staging.base import dict_to_elements\n",
    "from unstructured.chunking.title import chunk_by_title\n",
    "from  langchain.schema import Document\n",
    "from IPython.display import JSON, display, Markdown\n",
    "from langchain_community.vectorstores import FAISS\n",
    "# from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "# from  langchain.schema import Document\n",
    "import json\n",
    "from typing import Iterable\n",
    "import logging\n",
    "\n",
    "\n",
    "def process_pdfs_and_cache(input_folder, output_folder, strategy):\n",
    "    # Load environment variables from a .env file\n",
    "    load_dotenv()\n",
    "\n",
    "    # Get unstructured API key\n",
    "    unstructured_api_key = os.getenv(\"UNSTRUCTURED_API_KEY\")\n",
    "    if not unstructured_api_key:\n",
    "        raise ValueError(\"UNSTRUCTURED_API_KEY environment variable not found\")\n",
    "\n",
    "    # Initialize the UnstructuredClient\n",
    "    s = UnstructuredClient(api_key_auth=unstructured_api_key, server_url='https://redhorse-d652ahtg.api.unstructuredapp.io')\n",
    "\n",
    "    # Create cache folder if it doesn't exist\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Generate cache file path\n",
    "    folder_name = os.path.basename(os.path.normpath(input_folder))\n",
    "    cache_file_path = os.path.join(output_folder, f'{folder_name}_combined_content.json')\n",
    "\n",
    "    # Check if the combined content file already exists\n",
    "    if os.path.exists(cache_file_path):\n",
    "        print(f\"Loading combined content from {cache_file_path}...\")\n",
    "        with open(cache_file_path, 'r', encoding='utf-8') as f:\n",
    "            combined_content = json.load(f)\n",
    "    else:\n",
    "        # Initialize a list to hold the combined content\n",
    "        combined_content = []\n",
    "\n",
    "        # Iterate through all PDF files in the directory\n",
    "        for filename in glob.glob(os.path.join(input_folder, \"*.pdf\")):\n",
    "            print(f\"Processing {filename}...\")\n",
    "            with open(filename, \"rb\") as file:\n",
    "                req = shared.PartitionParameters(\n",
    "                    files=shared.Files(\n",
    "                        content=file.read(),\n",
    "                        file_name=filename,\n",
    "                    ),\n",
    "                    strategy=strategy,\n",
    "                )\n",
    "\n",
    "                try:\n",
    "                    res = s.general.partition(req)\n",
    "                    # Append the parsed elements to the combined content list\n",
    "                    combined_content.extend(res.elements)\n",
    "                except SDKError as e:\n",
    "                    print(f\"Error processing {filename}: {e}\")\n",
    "\n",
    "        # Display length of combined content\n",
    "        print(f\"Combined content length: {len(combined_content)}\")\n",
    "\n",
    "        # Save combined content to the cache file\n",
    "        with open(cache_file_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(combined_content, f)\n",
    "\n",
    "        print(f\"Combined content saved to {cache_file_path}\")\n",
    "\n",
    "    return combined_content\n",
    "# Function to process and chunk the data\n",
    "def process_data(combined_content):\n",
    "    pdf_elements = dict_to_elements(combined_content)\n",
    "    elements = chunk_by_title(pdf_elements, combine_text_under_n_chars=2000 ,  max_characters=5000, overlap=700)\n",
    "    # elements = chunk_elements(pdf_elements, max_characters=5000, overlap=1000)\n",
    "    documents = []\n",
    "    for element in elements:\n",
    "        metadata = element.metadata.to_dict()\n",
    "        del metadata[\"languages\"]\n",
    "        metadata[\"source\"] = metadata[\"filename\"]\n",
    "        documents.append(Document(page_content=element.text, metadata=metadata))\n",
    "    \n",
    "    return documents\n",
    "\n",
    "# Example usage:\n",
    "strategy = \"auto\" # \"auto\"\n",
    "combined_content = process_pdfs_and_cache(\"./data/far_seperatedParts\", \"./cache\", strategy)\n",
    "\n",
    "#PROCESS DATA\n",
    "print(f\"Chunking data...\")\n",
    "documents = process_data(combined_content)\n",
    "print(\"Finished chunking data\")\n",
    "\n",
    "# LOAD INTO VECTOR STORE\n",
    "# Initialize the OpenAIEmbeddings class\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Initialize the Chroma vector store\n",
    "#clear the vector store\n",
    "print(f\"Loading data into vector store\")\n",
    "vectorstore = FAISS.from_documents(documents, embeddings)\n",
    "print(\"Finish loading data into vector store\")\n",
    "\n",
    "# Create a retriever\n",
    "retriever = vectorstore.as_retriever(\n",
    "    # search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 6})\n",
    "\n",
    "query = \"1.102-2 Performance standards\"\n",
    "print(f\"Getting answer for: {query}\")\n",
    "answer = retriever.invoke(query)\n",
    "\n",
    "# Calculate the total length of all page_content\n",
    "total_length = sum(len(doc.page_content) for doc in answer)\n",
    "print(f\"Total length of all page_content combined: {total_length}\")\n",
    "\n",
    "answer\n",
    "\n",
    "#display each retrieve document for debugging\n",
    "for doc in answer:\n",
    "    display(Markdown(doc.page_content))\n",
    "    print(\"---------------------------------------------------\")\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted search keywords: 1.102-2 Performance standards\n",
      "Exact keyword match found by keyword search in document: Part 1 - Federal Acquisition Regulations System.pdf, Page: 5\n",
      "\n",
      "Organized text length: 10609\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Explanation of 1.102-2 Performance Standards\n",
       "\n",
       "The performance standards outlined in section 1.102-2 of the Federal Acquisition Regulations (FAR) emphasize the importance of satisfying the customer, minimizing administrative costs, conducting business with integrity, and fulfilling public policy objectives. These standards guide the acquisition process to ensure efficiency, fairness, and responsiveness to customer needs. For example, the government is encouraged to use contractors with a proven track record and to engage in early communication with industry to understand market capabilities.\n",
       "\n",
       "### Exact Wording from the Original Document\n",
       "\n",
       "#### 1.102-2 Performance standards.\n",
       "\n",
       "(a) Satisfy the customer in terms of cost, quality, and timeliness of the delivered product or service.\n",
       "\n",
       "1. The principal customers for the product or service provided by the System are the users and line managers, acting on behalf of the American taxpayer.\n",
       "\n",
       "2. The System must be responsive and adaptive to customer needs, concerns, and feedback. Implementation of acquisition policies and procedures, as well as consideration of timeliness, quality, and cost throughout the process, must take into account the perspective of the user of the product or service.\n",
       "\n",
       "3. When selecting contractors to provide products or perform services, the Government will use contractors who have a track record of successful past performance or who demonstrate a current superior ability to perform.\n",
       "\n",
       "4. The Government must not hesitate to communicate with industry as early as possible in the acquisition cycle to help the Government determine the capabilities available in the marketplace. Government acquisition personnel are permitted and encouraged to engage in responsible and constructive exchanges with industry (e.g., see 10.002 and 15.201), so long as those exchanges are consistent with existing laws and regulations, and do not promote an unfair competitive advantage to particular firms.\n",
       "\n",
       "5. The Government will maximize its use of commercial products and commercial services in meeting Government requirements.\n",
       "\n",
       "6. It is the policy of the System to promote competition in the acquisition process.\n",
       "\n",
       "7. The System must perform in a timely, high quality, and cost-effective manner.\n",
       "\n",
       "8. All members of the Team are required to employ planning as an integral part of the overall process of acquiring products or services. Although advance planning is required, each member of the Team must be flexible in order to accommodate changing or unforeseen mission needs. Planning is a tool for the accomplishment of tasks, and application of its discipline should be commensurate with the size and nature of a given task.\n",
       "\n",
       "(b) Minimize administrative operating costs.\n",
       "\n",
       "1. In order to ensure that maximum efficiency is obtained, rules, regulations, and policies should be promulgated only when their benefits clearly exceed the costs of their development, implementation, administration, and enforcement. This applies to internal administrative processes, including reviews, and to rules and procedures applied to the contractor community.\n",
       "\n",
       "2. The System must provide uniformity where it contributes to efficiency or where fairness or predictability is essential. The System should also, however, encourage innovation, and local adaptation where uniformity is not essential.\n",
       "\n",
       "(c) Conduct business with integrity, fairness, and openness.\n",
       "\n",
       "1. An essential consideration in every aspect of the System is maintaining the public’s trust. Not only must the System have integrity, but the actions of each member of the Team must reflect integrity, fairness, and openness. The foundation of integrity within the System is a competent, experienced, and well-trained, professional workforce. Accordingly, each member of the Team is responsible and accountable for the wise use of public resources as well as acting in a manner which maintains the public’s trust. Fairness and openness require open communication among team members, internal and external customers, and the public.\n",
       "\n",
       "2. To achieve efficient operations, the System must shift its focus from \"risk avoidance\" to one of \"risk management.\" The cost to the taxpayer of attempting to eliminate all risk is prohibitive. The Executive Branch will accept and manage the risk associated with empowering local procurement officials to take independent action based on their professional judgment.\n",
       "\n",
       "3. The Government shall exercise discretion, use sound business judgment, and comply with applicable laws and regulations in dealing with contractors and prospective contractors. All contractors and prospective contractors shall be treated fairly and impartially but need not be treated the same.\n",
       "\n",
       "(d) Fulfill public policy objectives. The System must support the attainment of public policy goals adopted by the Congress and the President. In attaining these goals, and in its overall operations, the process shall ensure the efficient use of public resources.\n",
       "\n",
       "### References to Other Sections or Clauses\n",
       "\n",
       "- **10.002**: Market research.\n",
       "- **15.201**: Exchanges with industry before receipt of proposals.\n",
       "- **42.302**: Contract administration functions."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from IPython.display import display, Markdown\n",
    "import os\n",
    "import warnings\n",
    "# from pydantic import PydanticDeprecatedSince20\n",
    "# warnings.filterwarnings(\"ignore\", category=PydanticDeprecatedSince20)\n",
    "import logging\n",
    "from rank_bm25 import BM25Okapi\n",
    "from typing import List, Tuple\n",
    "import re\n",
    "\n",
    "# Set logging level for httpx to WARNING or higher\n",
    "logging.getLogger(\"httpx\").setLevel(logging.WARNING)\n",
    "\n",
    "\n",
    "class EnhancedRetriever:\n",
    "    def __init__(self, vectorstore, documents):\n",
    "        self.vectorstore = vectorstore\n",
    "        self.documents = documents\n",
    "        self.bm25 = self._create_bm25_index()\n",
    "\n",
    "    def _create_bm25_index(self):\n",
    "        tokenized_docs = [self._tokenize(doc.page_content) for doc in self.documents]\n",
    "        return BM25Okapi(tokenized_docs)\n",
    "\n",
    "    def _tokenize(self, text: str) -> List[str]:\n",
    "        # Capture FAR section references and words\n",
    "        tokens = re.findall(r'\\b\\d+(?:\\.\\d+)*(?:\\s+[A-Za-z]+(?:\\s+[A-Za-z]+)*)?\\b|\\w+', text.lower())\n",
    "        return tokens\n",
    "\n",
    "    def exact_keyword_search(self, query: str) -> List[Tuple[float, str]]:\n",
    "        query_keywords = query.lower().split()\n",
    "        results = []\n",
    "        for doc in self.documents:\n",
    "            content = doc.page_content.lower()\n",
    "            score = sum(1 for keyword in query_keywords if keyword in content)\n",
    "            if score > 0:\n",
    "                results.append((score, doc))\n",
    "        return sorted(results, key=lambda x: x[0], reverse=True)\n",
    "    \n",
    "    def keyword_search(self, query: str, k: int = 4) -> List[Tuple[float, str]]:\n",
    "        tokenized_query = self._tokenize(query)\n",
    "        bm25_scores = self.bm25.get_scores(tokenized_query)\n",
    "        scored_docs = [(score, self.documents[i]) for i, score in enumerate(bm25_scores)]\n",
    "        return sorted(scored_docs, key=lambda x: x[0], reverse=True)[:k]\n",
    "        \n",
    "    def hybrid_search(self, query: str, k: int = 4) -> List[Tuple[float, str]]:\n",
    "        vector_results = self.vectorstore.similarity_search_with_score(query, k=k)\n",
    "        keyword_results = self.keyword_search(query, k=k)\n",
    "        \n",
    "        combined_results = {}\n",
    "        \n",
    "        # Convert query to lowercase and split into keywords\n",
    "        query_keywords = set(query.lower().split())\n",
    "        \n",
    "        for doc, score in vector_results:\n",
    "            combined_results[doc.page_content] = {'doc': doc, 'vector_score': score, 'keyword_score': 0, 'exact_match': False}\n",
    "            \n",
    "            # Check for exact keyword match\n",
    "            doc_words = set(doc.page_content.lower().split())\n",
    "            if query_keywords.issubset(doc_words):\n",
    "                combined_results[doc.page_content]['exact_match'] = True\n",
    "                print(f\"Exact keyword match found by similarity search in document: {doc.metadata.get('source', 'Unknown source')}, \"\n",
    "                    f\"Page: {doc.metadata.get('page_number', 'Unknown page')}\")\n",
    "        \n",
    "        for score, doc in keyword_results:\n",
    "            if doc.page_content in combined_results:\n",
    "                combined_results[doc.page_content]['keyword_score'] = score\n",
    "                if query_keywords.issubset(set(doc.page_content.lower().split())) and not combined_results[doc.page_content]['exact_match']:\n",
    "                    combined_results[doc.page_content]['exact_match'] = True\n",
    "                    print(f\"Exact keyword match found by keyword search in document: {doc.metadata.get('source', 'Unknown source')}, \"\n",
    "                        f\"Page: {doc.metadata.get('page_number', 'Unknown page')}\")\n",
    "            else:\n",
    "                combined_results[doc.page_content] = {'doc': doc, 'vector_score': 0, 'keyword_score': score, 'exact_match': False}\n",
    "                \n",
    "                # Check for exact keyword match for keyword results\n",
    "                doc_words = set(doc.page_content.lower().split())\n",
    "                if query_keywords.issubset(doc_words):\n",
    "                    combined_results[doc.page_content]['exact_match'] = True\n",
    "                    print(f\"Exact keyword match found by keyword search in document: {doc.metadata.get('source', 'Unknown source')}, \"\n",
    "                        f\"Page: {doc.metadata.get('page_number', 'Unknown page')}\")\n",
    "        \n",
    "        final_results = []\n",
    "        for content, scores in combined_results.items():\n",
    "            normalized_vector_score = 1 / (1 + scores['vector_score'])\n",
    "            normalized_keyword_score = scores['keyword_score']\n",
    "            exact_match_bonus = 2 if scores['exact_match'] else 0  # Increased the bonus for exact matches\n",
    "            combined_score = (normalized_vector_score + normalized_keyword_score + exact_match_bonus) / 3\n",
    "            final_results.append((combined_score, scores['doc']))\n",
    "        \n",
    "        # print(f\"Number of final results: {len(final_results)}\")\n",
    "        \n",
    "        return sorted(final_results, key=lambda x: x[0], reverse=True)[:k]  # Return only top k results\n",
    "\n",
    "def organize_documents(docs):\n",
    "    organized_text = \"\"\n",
    "    for i, doc in enumerate(docs, start=1):\n",
    "        source = doc.metadata.get('source', 'unknown source')\n",
    "        page_number = doc.metadata.get('page_number', 'unknown page number')\n",
    "        page_content = doc.page_content\n",
    "        organized_text += f\"Document {i}:\\n\"\n",
    "        organized_text += f\"Source: {source}\\n\"\n",
    "        organized_text += f\"Page number: {page_number}\\n\"\n",
    "        organized_text += f\"Content: {page_content}\\n\"\n",
    "        organized_text += \"\\n\\n\"\n",
    "    return organized_text\n",
    "\n",
    "def create_llm(model_name: str, streaming: bool = False):\n",
    "    if streaming:\n",
    "        return ChatOpenAI(\n",
    "            model_name=model_name,\n",
    "            temperature=0,\n",
    "            streaming=True,\n",
    "            callbacks=[StreamingStdOutCallbackHandler()]\n",
    "        )\n",
    "    else:\n",
    "        return ChatOpenAI(\n",
    "            model_name=model_name,\n",
    "            temperature=0\n",
    "        )\n",
    "\n",
    "\n",
    "def generate_answer_stream(query: str, relevant_data: str, llm: ChatOpenAI):\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"\n",
    "        Based on the following relevant data, please answer the user's query.\n",
    "        Provide a comprehensive and accurate answer, using the information given.\n",
    "        If the information is not sufficient to answer the query fully, state so clearly.\n",
    "        The answer always should have 2 parts. The first part should be the easy to understand answer/explaination and the second part should be the exact wording,make sure to get the full section, do not cut off(for citing purpose).\n",
    "\n",
    "\n",
    "        \n",
    "        Example:\n",
    "        Answer: Based on the provided data, here is a comprehensive explanation of the term \"1.103\" in the context of the Federal Acquisition Regulation (FAR):\n",
    "\n",
    "        FAR 1.103 - Authority\n",
    "        \n",
    "        Explanation:\n",
    "        FAR 1.103 outlines the authority under which the Federal Acquisition Regulation (FAR) system operates. It specifies that the authority to issue and maintain the FAR is shared among three key officials:\n",
    "\n",
    "        The Administrator of General Services\n",
    "        The Secretary of Defense\n",
    "        The Administrator of the National Aeronautics and Space Administration (NASA)\n",
    "        These officials operate under the broad policy guidance of the Office of Federal Procurement Policy (OFPP). The FAR is jointly prepared, issued, and maintained by these officials under their respective statutory authorities.\n",
    "\n",
    "        Exact Wording:\n",
    "        \"The authority of the FAR system is divided among the following: (a) The Administrator of General Services, the Secretary of Defense, and the Administrator of the National Aeronautics and Space Administration, under the broad policy guidance of the Office of Federal Procurement Policy, are authorized to issue and maintain the FAR. (b) The FAR is prepared, issued, and maintained, and the FAR System is prescribed jointly by the Secretary of Defense, the Administrator of General Services, and the Administrator of the National Aeronautics and Space Administration, under their several statutory authorities.\"\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        Relevant data:\n",
    "        {relevant_data}\n",
    "        \n",
    "        User query: {query}\n",
    "        \n",
    "        Answer:\n",
    "        \"\"\",\n",
    "        input_variables=[\"query\", \"relevant_data\"],\n",
    "    )\n",
    "    \n",
    "    # Create and invoke the chain with streaming\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    return chain.stream({\"query\": query, \"relevant_data\": relevant_data})\n",
    "\n",
    "\n",
    "def display_keyword_results(retriever: EnhancedRetriever, query: str, k: int = 4):\n",
    "    results = retriever.keyword_search(query, k)\n",
    "    print(f\"Keyword search results for query: '{query}'\\n\")\n",
    "    for i, (score, doc) in enumerate(results, 1):\n",
    "        print(f\"Result {i}:\")\n",
    "        print(f\"Score: {score}\")\n",
    "        print(f\"Source: {doc.metadata.get('source', 'unknown source')}\")\n",
    "        print(f\"Page number: {doc.metadata.get('page_number', 'unknown page number')}\")\n",
    "        print(f\"Content snippet: {doc.page_content}...\")  # Display first 200 characters\n",
    "        print(\"\\n\")\n",
    "\n",
    "def extract_search_keywords(query: str, llm: ChatOpenAI) -> str:\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"\n",
    "        Context: You are part of a system that processes user queries for searching through legal documents, specifically the Federal Acquisition Regulation (FAR). The system uses a hybrid search method that combines vector search and keyword search.\n",
    "\n",
    "        Task: Extract the most relevant keywords or phrases from the user's query that would be effective for searching. Focus on specific terms, section numbers, or phrases that are likely to yield the most relevant results.\n",
    "\n",
    "        Example:\n",
    "        User query: \"Tell me about the section 3.901 Definitions in exact wording, in a nice markdown format.\"\n",
    "        Extracted keywords: \"3.901 Definitions\"\n",
    "\n",
    "        Explanation: In this example, \"3.901 Definitions\" is extracted because it's a specific section reference that will yield precise results in the search system.\n",
    "\n",
    "        User query: {query}\n",
    "\n",
    "        Please extract the most relevant search keywords or phrases from this query. \n",
    "        Return your answer in JSON format with a single key \"keywords\".\n",
    "        Return only the json object with the key \"keywords\", no explanation, no json tag ```json,  is needed.\n",
    "\n",
    "        \"\"\",\n",
    "        input_variables=[\"query\"],\n",
    "    )\n",
    "    \n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    result = chain.invoke({\"query\": query})\n",
    "    # print(f\"Extract result: {result}\")\n",
    "    try:\n",
    "        extracted = json.loads(result)\n",
    "        return extracted[\"keywords\"]\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Error parsing LLM output. Using original query.\")\n",
    "        return query\n",
    "\n",
    "def rag_query_enhanced(user_query: str, enhanced_retriever: EnhancedRetriever, model_name: str = \"gpt-4o\", use_streaming: bool = False, k: int = 4):\n",
    "    # Create LLM instance for keyword extraction\n",
    "    keyword_llm = create_llm(model_name, streaming=False)\n",
    "    \n",
    "    # Extract search keywords\n",
    "    search_keywords = extract_search_keywords(user_query, keyword_llm)\n",
    "    print(f\"Extracted search keywords: {search_keywords}\")\n",
    "    \n",
    "    # Retrieve relevant documents using the enhanced retriever with extracted keywords\n",
    "    retrieved_docs = enhanced_retriever.hybrid_search(search_keywords, k=k)\n",
    "    \n",
    "    # Organize retrieved documents\n",
    "    organized_text = organize_documents([doc for _, doc in retrieved_docs])\n",
    "    print(f\"\\nOrganized text length: {len(organized_text)}\")\n",
    "    \n",
    "    # Create LLM instance for generating the answer\n",
    "    answer_llm = create_llm(model_name, use_streaming)\n",
    "    \n",
    "    if use_streaming:\n",
    "        # Generate answer using LLM with streaming\n",
    "        for chunk in generate_answer_stream(user_query, organized_text, answer_llm):\n",
    "            print(chunk, end=\"\", flush=True)\n",
    "    else:\n",
    "        # Generate answer using LLM without streaming\n",
    "        answer = generate_answer(user_query, organized_text, answer_llm)\n",
    "        display(Markdown(answer))\n",
    "\n",
    "def generate_answer(query: str, relevant_data: str, llm: ChatOpenAI):\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"\n",
    "        #Relevant data:\n",
    "        {relevant_data}\n",
    "        \n",
    "        \\n\\n\n",
    "        #Instruction:\n",
    "        - Based on the following relevant data, please answer the user's query.\n",
    "        - Provide a comprehensive and accurate answer, using the information given.\n",
    "        - If the information is not sufficient to answer the query fully, state so clearly.\n",
    "        - Sometime, the full section got cut off or break down to separated parts. Carefully look at the relevant data and connect them if they belong to each other. Pay attention to the sections number to figure it out.\n",
    "        -The answer always should have 3 parts:\n",
    "          - The first part should be the easy to understand answer/explanation. With a concise, short example. \n",
    "          - the second part should be the exact wording and format to the original doc, make sure to get the full section, do not cut off(for citing purpose).\n",
    "          - In the third part, if the content refers to any other section or clause(s), please state it out to the user. \n",
    "        \n",
    "        #User query: {query}\n",
    "        \n",
    "        #Answer:\n",
    "        \"\"\",\n",
    "        input_variables=[\"query\", \"relevant_data\"],\n",
    "    )\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    return chain.invoke({\"query\": query, \"relevant_data\": relevant_data})\n",
    "\n",
    "# Usage example:\n",
    "enhanced_retriever = EnhancedRetriever(vectorstore, documents)\n",
    "\n",
    "# #Test search function:\n",
    "# print(f\"Query: {query}\")\n",
    "# answer = enhanced_retriever.hybrid_search(query, k=4)\n",
    "# #display each retrieve document for debugging\n",
    "# for doc in answer:\n",
    "#     display(Markdown(doc[1].page_content))\n",
    "    \n",
    "# cant find without keyword search:\n",
    "# 1.102-2 Performance standards.\n",
    "# 1.102-1 Discussion.\n",
    "# 1.102 Statement of guiding principles for the Federal Acquisition System.\n",
    "# 3.901 Definitions.\n",
    "# 3.902 Classified information.\n",
    "\n",
    "query = \"\"\"tell me about the 1.102-2 Performance standards,\n",
    "output it nicely in markdown format.\n",
    "\"\"\"\n",
    "\n",
    "rag_query_enhanced(query, enhanced_retriever, model_name=\"gpt-4o\", use_streaming=False, k=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "farchat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
