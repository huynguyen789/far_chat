{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install google-generativeai ipywidgets pandas openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note:\n",
    "- Many errors with api\n",
    "- Super slow speed: ~90s for each answer\n",
    "- Major bugs: sometimes doesn't return any answer. IOMGR endpoint shutdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemini only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get answer from the gemini cache\n",
    "save into a jsonl file\n",
    "\n",
    "Create evaluation function\n",
    "- Take in testDataset: with Q&A jsonl format\n",
    "- Take in the model answers in jsonl format\n",
    "- Create evaluation matrix\n",
    "- Compare and evaluate the model answers with the testDataset answers, using openai api, get back a reasoning and score.\n",
    "- Organize them into a df with test question, test answer, model answer, score, reasoning\n",
    "- Export df into an excel file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1720724062.718382 3541301 tcp_posix.cc:809] IOMGR endpoint shutdown\n",
      "I0000 00:00:1720724062.732190 3477388 check_gcp_environment_no_op.cc:29] ALTS: Platforms other than Linux and Windows are not supported\n",
      "I0000 00:00:1720724093.396321 3610849 tcp_posix.cc:809] IOMGR endpoint shutdown\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The\n",
      " FAR 47.403-3 Disallowance of Expenditures states:\n",
      "\n",
      "\n",
      "**(a)** Agencies shall disallow expenditures for U.S. Government-\n",
      "financed commercial international air transportation on foreign-flag air carriers unless there is attached to the appropriate voucher a memorandum adequately explaining why service by U.S.-flag\n",
      " air carriers was not available, or why it was necessary to use foreign-flag air carriers.\n",
      "\n",
      "**(b)** When the travel is by indirect route or the\n",
      " traveler otherwise fails to use available U.S.-flag air carrier service, the amount to be disallowed against the traveler is based on the loss of revenues suffered by U.S.-flag air carriers as determined under the following formula, which is\n",
      " prescribed and more fully explained in 56 Comp. Gen. 209 (1977):\n",
      "\n",
      "**(c)** The justification requirement is satisfied by the contractor's use of a statement similar to the one contained in the\n",
      " clause at \\cf3 52.247-63\\cf2 , Preference for U.S.-Flag Air Carriers. (See \\cf3 47.405\\cf2 ). \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1720724407.147536 3541293 tcp_posix.cc:809] IOMGR endpoint shutdown\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
    "\n",
    "\n",
    "#load file:\n",
    "text = \"\"\n",
    "with open('/Users/huyknguyen/Desktop/redhorse/code_projects/far_chat/docs/FAR_28-39.rtf', 'r') as file:\n",
    "    text = file.read()\n",
    "    \n",
    "\n",
    "# Create the model\n",
    "# See https://ai.google.dev/api/python/google/generativeai/GenerativeModel\n",
    "generation_config = {\n",
    "  \"temperature\": 0,\n",
    "  \"top_p\": 0.95,\n",
    "  \"top_k\": 64,\n",
    "  \"max_output_tokens\": 8192,\n",
    "  \"response_mime_type\": \"text/plain\",\n",
    "}\n",
    "\n",
    "model = genai.GenerativeModel(\n",
    "  model_name=\"gemini-1.5-flash\",\n",
    "  generation_config=generation_config, \n",
    "  # content=text,\n",
    "  # safety_settings = Adjust safety settings\n",
    "  # See https://ai.google.dev/gemini-api/docs/safety-settings\n",
    "  system_instruction=\"You are an expert on the Federal Acquisition Regulation (FAR). \\nYour task is to answer user queries based on the FAR document content provided. \\nAlways provide accurate information and cite the relevant FAR sections when possible.\",\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "response = model.generate_content(f\"Based on this file:{text}.\\n\\n Tell me about the 47.403-3 Disallowance of expenditures, in verbatim:\", stream=True)\n",
    "for chunk in response:\n",
    "  print(chunk.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=protos.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"content\": {\n",
       "            \"parts\": [\n",
       "              {\n",
       "                \"text\": \"The FAR 47.403-3 Disallowance of Expenditures states:\\n\\n**(a)** Agencies shall disallow expenditures for U.S. Government-financed commercial international air transportation on foreign-flag air carriers unless there is attached to the appropriate voucher a memorandum adequately explaining why service by U.S.-flag air carriers was not available, or why it was necessary to use foreign-flag air carriers.\\n\\n**(b)** When the travel is by indirect route or the traveler otherwise fails to use available U.S.-flag air carrier service, the amount to be disallowed against the traveler is based on the loss of revenues suffered by U.S.-flag air carriers as determined under the following formula, which is prescribed and more fully explained in 56 Comp. Gen. 209 (1977):\\n\\n**(c)** The justification requirement is satisfied by the contractor's use of a statement similar to the one contained in the clause at \\\\cf3 52.247-63\\\\cf2 , Preference for U.S.-Flag Air Carriers. (See \\\\cf3 47.405\\\\cf2 ). \\n\"\n",
       "              }\n",
       "            ],\n",
       "            \"role\": \"model\"\n",
       "          },\n",
       "          \"finish_reason\": \"STOP\",\n",
       "          \"index\": 0,\n",
       "          \"safety_ratings\": [\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            }\n",
       "          ],\n",
       "          \"citation_metadata\": {\n",
       "            \"citation_sources\": [\n",
       "              {\n",
       "                \"start_index\": 765,\n",
       "                \"end_index\": 894,\n",
       "                \"uri\": \"http://federal.elaws.us/cfr/title48.part47.section47.403-3\",\n",
       "                \"license_\": \"\"\n",
       "              }\n",
       "            ]\n",
       "          }\n",
       "        }\n",
       "      ],\n",
       "      \"prompt_feedback\": {},\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 1837117,\n",
       "        \"candidates_token_count\": 239,\n",
       "        \"total_token_count\": 1837356\n",
       "      }\n",
       "    }),\n",
       ")"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemini with cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAR document loaded successfully. Document length: 7021793 characters.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1720723276.659246 3477388 check_gcp_environment_no_op.cc:29] ALTS: Platforms other than Linux and Windows are not supported\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache created successfully.\n",
      "Model with cache initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "from google.generativeai import caching\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "# Configure the Gemini API\n",
    "genai.configure(api_key=os.environ['GOOGLE_API_KEY'])\n",
    "\n",
    "# Global variables\n",
    "far_doc_path = None\n",
    "model_name = 'models/gemini-1.5-pro-001'\n",
    "cache_ttl = datetime.timedelta(hours=1)\n",
    "cache = None\n",
    "model = None\n",
    "\n",
    "safety_settings = [\n",
    "    {\n",
    "        \"category\": \"HARM_CATEGORY_DANGEROUS\",\n",
    "        \"threshold\": \"BLOCK_NONE\",\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
    "        \"threshold\": \"BLOCK_NONE\",\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
    "        \"threshold\": \"BLOCK_NONE\",\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
    "        \"threshold\": \"BLOCK_NONE\",\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "        \"threshold\": \"BLOCK_NONE\",\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "def initialize_cache(far_doc_path, model_name='models/gemini-1.5-flash-001', cache_ttl=datetime.timedelta(hours=1)):\n",
    "    global cache, model\n",
    "    \n",
    "    # Read the FAR document\n",
    "    with open(far_doc_path, 'r', encoding='utf-8') as file:\n",
    "        far_content = file.read()\n",
    "    print(f\"FAR document loaded successfully. Document length: {len(far_content)} characters.\")\n",
    "\n",
    "    # Create a cache\n",
    "    cache = caching.CachedContent.create(\n",
    "        model=model_name,\n",
    "        display_name='FAR Document Cache',\n",
    "\n",
    "        system_instruction=(\n",
    "            \"You are an expert on the Federal Acquisition Regulation (FAR). \"\n",
    "            \"Your task is to answer user queries based on the FAR document content provided. \"\n",
    "            \"Always provide accurate information and cite the relevant FAR sections when possible.\"\n",
    "        ),\n",
    "        contents=[far_content],\n",
    "        ttl=cache_ttl,\n",
    "    )\n",
    "    print(f\"Cache created successfully.\")\n",
    "\n",
    "    # Construct a GenerativeModel which uses the created cache\n",
    "    model = genai.GenerativeModel.from_cached_content(cached_content=cache, safety_settings=safety_settings)\n",
    "    print(\"Model with cache initialized successfully.\")\n",
    "\n",
    "def ask_question(question):\n",
    "    if not model:\n",
    "        raise ValueError(\"Cache not initialized. Call initialize_cache() first.\")\n",
    "    \n",
    "    print(f\"Getting answer for question: {question}\")\n",
    "    response = model.generate_content(question)\n",
    "    print(f\"Response: {response}\")\n",
    "    print(f\"Response text: {response.text}\")\n",
    "    print(\"Token Usage:\")\n",
    "    print(f\"Prompt tokens: {response.usage_metadata.prompt_token_count}\")\n",
    "    print(f\"Cached tokens: {response.usage_metadata.cached_content_token_count}\")\n",
    "    print(f\"Response tokens: {response.usage_metadata.candidates_token_count}\")\n",
    "    print(f\"Total tokens: {response.usage_metadata.total_token_count}\")\n",
    "    return response.text\n",
    "\n",
    "def update_cache_ttl(new_ttl):\n",
    "    global cache\n",
    "    if cache:\n",
    "        cache.update(ttl=new_ttl)\n",
    "        print(f\"Cache TTL updated to {new_ttl}\")\n",
    "    else:\n",
    "        print(\"Cache not initialized yet.\")\n",
    "\n",
    "def delete_cache():\n",
    "    global cache, model\n",
    "    if cache:\n",
    "        cache.delete()\n",
    "        cache = None\n",
    "        model = None\n",
    "        print(\"Cache deleted successfully.\")\n",
    "    else:\n",
    "        print(\"No cache to delete.\")\n",
    "\n",
    "# Example usage\n",
    "far_path = \"/Users/huyknguyen/Desktop/redhorse/code_projects/far_chat/docs/FAR.rtf\"\n",
    "initialize_cache(far_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model:\n",
    "    raise ValueError(\"Cache not initialized. Call initialize_cache() first.\")\n",
    "\n",
    "question = \"Tell me about the 1.201-1 The two councils in verbatim:\"\n",
    "\n",
    "\n",
    "print(f\"Getting answer for question: {question}\")\n",
    "response = model.generate_content(question)\n",
    "\n",
    "print(f\"Response: {response}\")\n",
    "print(f\"Response text: {response.text}\")\n",
    "print(\"Token Usage:\")\n",
    "print(f\"Prompt tokens: {response.usage_metadata.prompt_token_count}\")\n",
    "print(f\"Cached tokens: {response.usage_metadata.cached_content_token_count}\")\n",
    "print(f\"Response tokens: {response.usage_metadata.candidates_token_count}\")\n",
    "print(f\"Total tokens: {response.usage_metadata.total_token_count}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1720723303.286800 3477388 check_gcp_environment_no_op.cc:29] ALTS: Platforms other than Linux and Windows are not supported\n",
      "I0000 00:00:1720723351.391640 3541348 tcp_posix.cc:809] IOMGR endpoint shutdown\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid operation: The `response.text` quick accessor requires the response to contain a valid `Part`, but none were returned. Please check the `candidate.safety_ratings` to determine if the response was blocked.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[77], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m response \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate_content(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTell me about the 1.201-1 The two councils section in verbatim\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m display(Markdown(\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m))\n",
      "File \u001b[0;32m~/Desktop/redhorse/code_projects/far_chat/.conda/lib/python3.11/site-packages/google/generativeai/types/generation_types.py:436\u001b[0m, in \u001b[0;36mBaseGenerateContentResponse.text\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    434\u001b[0m parts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparts\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parts:\n\u001b[0;32m--> 436\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid operation: The `response.text` quick accessor requires the response to contain a valid `Part`, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    438\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut none were returned. Please check the `candidate.safety_ratings` to determine if the response was blocked.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    439\u001b[0m     )\n\u001b[1;32m    441\u001b[0m texts \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m parts:\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid operation: The `response.text` quick accessor requires the response to contain a valid `Part`, but none were returned. Please check the `candidate.safety_ratings` to determine if the response was blocked."
     ]
    }
   ],
   "source": [
    "response = model.generate_content(\"Tell me about the 1.201-1 The two councils section in verbatim\")\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "from google.generativeai import caching\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "# Configure the Gemini API\n",
    "genai.configure(api_key=os.environ['GOOGLE_API_KEY'])\n",
    "\n",
    "class FARChatbot:\n",
    "    def __init__(self, far_doc_path, model_name='models/gemini-1.5-flash-001', cache_ttl=datetime.timedelta(hours=1)):\n",
    "        self.far_doc_path = far_doc_path\n",
    "        self.model_name = model_name\n",
    "        self.cache_ttl = cache_ttl\n",
    "        self.cache = None\n",
    "        self.model = None\n",
    "\n",
    "    def initialize_cache(self):\n",
    "        # Read the FAR document\n",
    "        with open(self.far_doc_path, 'r', encoding='utf-8') as file:\n",
    "            far_content = file.read()\n",
    "            print(f\"FAR document loaded successfully. Document length: {len(far_content)} characters.\")\n",
    "\n",
    "        # Create a cache\n",
    "        self.cache = caching.CachedContent.create(\n",
    "            model=self.model_name,\n",
    "            display_name='FAR Document Cache',\n",
    "            system_instruction=(\n",
    "                \"You are an expert on the Federal Acquisition Regulation (FAR). \"\n",
    "                \"Your task is to answer user queries based on the FAR document content provided. \"\n",
    "                \"Always provide accurate information and cite the relevant FAR sections when possible.\"\n",
    "            ),\n",
    "            contents=[far_content],\n",
    "            ttl=self.cache_ttl,\n",
    "        )\n",
    "        print(f\"Cache created successfully.\")\n",
    "\n",
    "        # Construct a GenerativeModel which uses the created cache\n",
    "        self.model = genai.GenerativeModel.from_cached_content(cached_content=self.cache)\n",
    "        print(\"Model with cache initialized successfully.\")\n",
    "\n",
    "    def ask_question(self, question):\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Cache not initialized. Call initialize_cache() first.\")\n",
    "        print(f\"Getting answer for question: {question}\")\n",
    "        response = self.model.generate_content(question)\n",
    "        print(f\"Response: {response}\")\n",
    "        print(f\"Response text: {response.text}\")\n",
    "        \n",
    "        print(\"Token Usage:\")\n",
    "        print(f\"Prompt tokens: {response.usage_metadata.prompt_token_count}\")\n",
    "        print(f\"Cached tokens: {response.usage_metadata.cached_content_token_count}\")\n",
    "        print(f\"Response tokens: {response.usage_metadata.candidates_token_count}\")\n",
    "        print(f\"Total tokens: {response.usage_metadata.total_token_count}\")\n",
    "\n",
    "        return response.text\n",
    "\n",
    "    def update_cache_ttl(self, new_ttl):\n",
    "        if self.cache:\n",
    "            self.cache.update(ttl=new_ttl)\n",
    "            print(f\"Cache TTL updated to {new_ttl}\")\n",
    "        else:\n",
    "            print(\"Cache not initialized yet.\")\n",
    "\n",
    "    def delete_cache(self):\n",
    "        if self.cache:\n",
    "            self.cache.delete()\n",
    "            self.cache = None\n",
    "            self.model = None\n",
    "            print(\"Cache deleted successfully.\")\n",
    "        else:\n",
    "            print(\"No cache to delete.\")\n",
    "\n",
    "# Example usage\n",
    "far_path = \"/Users/huyknguyen/Desktop/redhorse/code_projects/far_chat/docs/FAR.rtf\"\n",
    "chatbot = FARChatbot(far_path)\n",
    "\n",
    "chatbot.initialize_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"Tell me about the 1.201-1 The two councils in verbatim:\"\n",
    "\n",
    "\n",
    "print(f\"Getting answer for question: {question}\")\n",
    "response = self.model.generate_content(question)\n",
    "print(f\"Response: {response}\")\n",
    "print(f\"Response text: {response.text}\")\n",
    "\n",
    "\n",
    "print(\"Answer:\", answer)\n",
    "print()\n",
    "\n",
    "# chatbot.delete_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def parse_json_object(lines):\n",
    "    obj = {}\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line.startswith('\"Q\":'):\n",
    "            obj['Q'] = line.split('\"Q\":', 1)[1].strip().strip('\"')\n",
    "        elif line.startswith('\"A\":'):\n",
    "            obj['A'] = line.split('\"A\":', 1)[1].strip().strip('\"')\n",
    "    return obj if obj else None\n",
    "\n",
    "# def process_testdata(chatbot, input_file, output_file):\n",
    "with open(input_file, 'r', encoding='utf-8') as infile, open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "    content = infile.read()\n",
    "    \n",
    "    # Split the content into individual JSON-like objects\n",
    "    json_objects = content.split('}\\n{')\n",
    "    \n",
    "    for i, obj in enumerate(json_objects):\n",
    "        # Add back the curly braces that were removed in the split\n",
    "        if i > 0:\n",
    "            obj = '{' + obj\n",
    "        if i < len(json_objects) - 1:\n",
    "            obj += '}'\n",
    "        \n",
    "        parsed = parse_json_object(obj.split('\\n'))\n",
    "        if parsed:\n",
    "            question = parsed.get('Q', '')\n",
    "            print(f\"\\nParsed question: {question}...\")\n",
    "            if question:\n",
    "                answer = chatbot.ask_question(question)\n",
    "                print(f\"\\nAnswer: {answer[:50]}...\")\n",
    "                result = {\n",
    "                    'question': question,\n",
    "                    'test_answer': parsed.get('A', ''),\n",
    "                    'model_answer': answer\n",
    "                }\n",
    "                json.dump(result, outfile, ensure_ascii=False)\n",
    "                outfile.write('\\n')\n",
    "                print(f\"Processed question: {question[:50]}...\")\n",
    "        else:\n",
    "            print(f\"Error parsing object {i+1}\")\n",
    "\n",
    "input_file = \"/Users/huyknguyen/Desktop/redhorse/code_projects/far_chat/docs/testData.jsonl\"\n",
    "output_file = \"/Users/huyknguyen/Desktop/redhorse/code_projects/far_chat/docs/testDataAnswers.jsonl\"\n",
    "\n",
    "process_testdata(chatbot, input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = input(\"Ask a question about FAR (or type 'exit' to quit): \")\n",
    "# if user_input.lower() == 'exit' or 'q':\n",
    "#     break\n",
    "\n",
    "answer = chatbot.ask_question(user_input)\n",
    "print(\"Answer:\", answer)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get answer from the gemini cache\n",
    "save into a jsonl file\n",
    "\n",
    "Create evaluation function\n",
    "- Take in testDataset: with Q&A jsonl format\n",
    "- Take in the model answers in jsonl format\n",
    "- Create evaluation matrix\n",
    "- Compare and evaluate the model answers with the testDataset answers, using openai api, get back a reasoning and score.\n",
    "- Organize them into a df with test question, test answer, model answer, score, reasoning\n",
    "- Export df into an excel file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
